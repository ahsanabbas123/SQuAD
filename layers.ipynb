{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Assortment of layers for use in models.py.\n",
        "\n",
        "Author:\n",
        "    Chris Chute (chute@stanford.edu)\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from util import masked_softmax\n",
        "\n",
        "\n",
        "class Embedding(nn.Module):\n",
        "    \"\"\"Embedding layer used by BiDAF, without the character-level component.\n",
        "\n",
        "    Word-level embeddings are further refined using a 2-layer Highway Encoder\n",
        "    (see `HighwayEncoder` class for details).\n",
        "\n",
        "    Args:\n",
        "        word_vectors (torch.Tensor): Pre-trained word vectors.\n",
        "        hidden_size (int): Size of hidden activations.\n",
        "        drop_prob (float): Probability of zero-ing out activations\n",
        "    \"\"\"\n",
        "    def __init__(self, word_vectors, hidden_size, drop_prob):\n",
        "        super(Embedding, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.embed = nn.Embedding.from_pretrained(word_vectors)\n",
        "        self.proj = nn.Linear(word_vectors.size(1), hidden_size, bias=False)\n",
        "        self.hwy = HighwayEncoder(2, hidden_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embed(x)   # (batch_size, seq_len, embed_size)\n",
        "        emb = F.dropout(emb, self.drop_prob, self.training)\n",
        "        emb = self.proj(emb)  # (batch_size, seq_len, hidden_size)\n",
        "        emb = self.hwy(emb)   # (batch_size, seq_len, hidden_size)\n",
        "\n",
        "        return emb\n",
        "\n",
        "\n",
        "class HighwayEncoder(nn.Module):\n",
        "    \"\"\"Encode an input sequence using a highway network.\n",
        "\n",
        "    Based on the paper:\n",
        "    \"Highway Networks\"\n",
        "    by Rupesh Kumar Srivastava, Klaus Greff, J\u00c3\u00bcrgen Schmidhuber\n",
        "    (https://arxiv.org/abs/1505.00387).\n",
        "\n",
        "    Args:\n",
        "        num_layers (int): Number of layers in the highway encoder.\n",
        "        hidden_size (int): Size of hidden activations.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_layers, hidden_size):\n",
        "        super(HighwayEncoder, self).__init__()\n",
        "        self.transforms = nn.ModuleList([nn.Linear(hidden_size, hidden_size)\n",
        "                                         for _ in range(num_layers)])\n",
        "        self.gates = nn.ModuleList([nn.Linear(hidden_size, hidden_size)\n",
        "                                    for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for gate, transform in zip(self.gates, self.transforms):\n",
        "            # Shapes of g, t, and x are all (batch_size, seq_len, hidden_size)\n",
        "            g = torch.sigmoid(gate(x))\n",
        "            t = F.relu(transform(x))\n",
        "            x = g * t + (1 - g) * x\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class RNNEncoder(nn.Module):\n",
        "    \"\"\"General-purpose layer for encoding a sequence using a bidirectional RNN.\n",
        "\n",
        "    Encoded output is the RNN's hidden state at each position, which\n",
        "    has shape `(batch_size, seq_len, hidden_size * 2)`.\n",
        "\n",
        "    Args:\n",
        "        input_size (int): Size of a single timestep in the input.\n",
        "        hidden_size (int): Size of the RNN hidden state.\n",
        "        num_layers (int): Number of layers of RNN cells to use.\n",
        "        drop_prob (float): Probability of zero-ing out activations.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 input_size,\n",
        "                 hidden_size,\n",
        "                 num_layers,\n",
        "                 drop_prob=0.):\n",
        "        super(RNNEncoder, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers,\n",
        "                           batch_first=True,\n",
        "                           bidirectional=True,\n",
        "                           dropout=drop_prob if num_layers > 1 else 0.)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        # Save original padded length for use by pad_packed_sequence\n",
        "        orig_len = x.size(1)\n",
        "\n",
        "        # Sort by length and pack sequence for RNN\n",
        "        lengths, sort_idx = lengths.sort(0, descending=True)\n",
        "        x = x[sort_idx]     # (batch_size, seq_len, input_size)\n",
        "        x = pack_padded_sequence(x, lengths, batch_first=True)\n",
        "\n",
        "        # Apply RNN\n",
        "        x, _ = self.rnn(x)  # (batch_size, seq_len, 2 * hidden_size)\n",
        "\n",
        "        # Unpack and reverse sort\n",
        "        x, _ = pad_packed_sequence(x, batch_first=True, total_length=orig_len)\n",
        "        _, unsort_idx = sort_idx.sort(0)\n",
        "        x = x[unsort_idx]   # (batch_size, seq_len, 2 * hidden_size)\n",
        "\n",
        "        # Apply dropout (RNN applies dropout after all but the last layer)\n",
        "        x = F.dropout(x, self.drop_prob, self.training)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class BiDAFAttention(nn.Module):\n",
        "    \"\"\"Bidirectional attention originally used by BiDAF.\n",
        "\n",
        "    Bidirectional attention computes attention in two directions:\n",
        "    The context attends to the query and the query attends to the context.\n",
        "    The output of this layer is the concatenation of [context, c2q_attention,\n",
        "    context * c2q_attention, context * q2c_attention]. This concatenation allows\n",
        "    the attention vector at each timestep, along with the embeddings from\n",
        "    previous layers, to flow through the attention layer to the modeling layer.\n",
        "    The output has shape (batch_size, context_len, 8 * hidden_size).\n",
        "\n",
        "    Args:\n",
        "        hidden_size (int): Size of hidden activations.\n",
        "        drop_prob (float): Probability of zero-ing out activations.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size, drop_prob=0.1):\n",
        "        super(BiDAFAttention, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.c_weight = nn.Parameter(torch.zeros(hidden_size, 1))\n",
        "        self.q_weight = nn.Parameter(torch.zeros(hidden_size, 1))\n",
        "        self.cq_weight = nn.Parameter(torch.zeros(1, 1, hidden_size))\n",
        "        for weight in (self.c_weight, self.q_weight, self.cq_weight):\n",
        "            nn.init.xavier_uniform_(weight)\n",
        "        self.bias = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, c, q, c_mask, q_mask):\n",
        "        batch_size, c_len, _ = c.size()\n",
        "        q_len = q.size(1)\n",
        "        s = self.get_similarity_matrix(c, q)        # (batch_size, c_len, q_len)\n",
        "        c_mask = c_mask.view(batch_size, c_len, 1)  # (batch_size, c_len, 1)\n",
        "        q_mask = q_mask.view(batch_size, 1, q_len)  # (batch_size, 1, q_len)\n",
        "        s1 = masked_softmax(s, q_mask, dim=2)       # (batch_size, c_len, q_len)\n",
        "        s2 = masked_softmax(s, c_mask, dim=1)       # (batch_size, c_len, q_len)\n",
        "\n",
        "        # (bs, c_len, q_len) x (bs, q_len, hid_size) => (bs, c_len, hid_size)\n",
        "        a = torch.bmm(s1, q)\n",
        "        # (bs, c_len, c_len) x (bs, c_len, hid_size) => (bs, c_len, hid_size)\n",
        "        b = torch.bmm(torch.bmm(s1, s2.transpose(1, 2)), c)\n",
        "\n",
        "        x = torch.cat([c, a, c * a, c * b], dim=2)  # (bs, c_len, 4 * hid_size)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def get_similarity_matrix(self, c, q):\n",
        "        \"\"\"Get the \"similarity matrix\" between context and query (using the\n",
        "        terminology of the BiDAF paper).\n",
        "\n",
        "        A naive implementation as described in BiDAF would concatenate the\n",
        "        three vectors then project the result with a single weight matrix. This\n",
        "        method is a more memory-efficient implementation of the same operation.\n",
        "\n",
        "        See Also:\n",
        "            Equation 1 in https://arxiv.org/abs/1611.01603\n",
        "        \"\"\"\n",
        "        c_len, q_len = c.size(1), q.size(1)\n",
        "        c = F.dropout(c, self.drop_prob, self.training)  # (bs, c_len, hid_size)\n",
        "        q = F.dropout(q, self.drop_prob, self.training)  # (bs, q_len, hid_size)\n",
        "\n",
        "        # Shapes: (batch_size, c_len, q_len)\n",
        "        s0 = torch.matmul(c, self.c_weight).expand([-1, -1, q_len])\n",
        "        s1 = torch.matmul(q, self.q_weight).transpose(1, 2)\\\n",
        "                                           .expand([-1, c_len, -1])\n",
        "        s2 = torch.matmul(c * self.cq_weight, q.transpose(1, 2))\n",
        "        s = s0 + s1 + s2 + self.bias\n",
        "\n",
        "        return s\n",
        "\n",
        "\n",
        "class BiDAFOutput(nn.Module):\n",
        "    \"\"\"Output layer used by BiDAF for question answering.\n",
        "\n",
        "    Computes a linear transformation of the attention and modeling\n",
        "    outputs, then takes the softmax of the result to get the start pointer.\n",
        "    A bidirectional LSTM is then applied the modeling output to produce `mod_2`.\n",
        "    A second linear+softmax of the attention output and `mod_2` is used\n",
        "    to get the end pointer.\n",
        "\n",
        "    Args:\n",
        "        hidden_size (int): Hidden size used in the BiDAF model.\n",
        "        drop_prob (float): Probability of zero-ing out activations.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size, drop_prob):\n",
        "        super(BiDAFOutput, self).__init__()\n",
        "        self.att_linear_1 = nn.Linear(8 * hidden_size, 1)\n",
        "        self.mod_linear_1 = nn.Linear(2 * hidden_size, 1)\n",
        "\n",
        "        self.rnn = RNNEncoder(input_size=2 * hidden_size,\n",
        "                              hidden_size=hidden_size,\n",
        "                              num_layers=1,\n",
        "                              drop_prob=drop_prob)\n",
        "\n",
        "        self.att_linear_2 = nn.Linear(8 * hidden_size, 1)\n",
        "        self.mod_linear_2 = nn.Linear(2 * hidden_size, 1)\n",
        "\n",
        "    def forward(self, att, mod, mask):\n",
        "        # Shapes: (batch_size, seq_len, 1)\n",
        "        logits_1 = self.att_linear_1(att) + self.mod_linear_1(mod)\n",
        "        mod_2 = self.rnn(mod, mask.sum(-1))\n",
        "        logits_2 = self.att_linear_2(att) + self.mod_linear_2(mod_2)\n",
        "\n",
        "        # Shapes: (batch_size, seq_len)\n",
        "        log_p1 = masked_softmax(logits_1.squeeze(), mask, log_softmax=True)\n",
        "        log_p2 = masked_softmax(logits_2.squeeze(), mask, log_softmax=True)\n",
        "\n",
        "        return log_p1, log_p2"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}