{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Download and pre-process SQuAD and GloVe.\n",
        "\n",
        "Usage:\n",
        "    > source activate squad\n",
        "    > python setup.py\n",
        "\n",
        "Pre-processing code adapted from:\n",
        "    > https://github.com/HKUST-KnowComp/R-Net/blob/master/prepro.py\n",
        "\n",
        "Author:\n",
        "    Chris Chute (chute@stanford.edu)\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import spacy\n",
        "import ujson as json\n",
        "import urllib.request\n",
        "\n",
        "from args import get_setup_args\n",
        "from codecs import open\n",
        "from collections import Counter\n",
        "from subprocess import run\n",
        "from tqdm import tqdm\n",
        "from zipfile import ZipFile\n",
        "\n",
        "\n",
        "def download_url(url, output_path, show_progress=True):\n",
        "    class DownloadProgressBar(tqdm):\n",
        "        def update_to(self, b=1, bsize=1, tsize=None):\n",
        "            if tsize is not None:\n",
        "                self.total = tsize\n",
        "            self.update(b * bsize - self.n)\n",
        "\n",
        "    if show_progress:\n",
        "        # Download with a progress bar\n",
        "        with DownloadProgressBar(unit='B', unit_scale=True,\n",
        "                                 miniters=1, desc=url.split('/')[-1]) as t:\n",
        "            urllib.request.urlretrieve(url,\n",
        "                                       filename=output_path,\n",
        "                                       reporthook=t.update_to)\n",
        "    else:\n",
        "        # Simple download with no progress bar\n",
        "        urllib.request.urlretrieve(url, output_path)\n",
        "\n",
        "\n",
        "def url_to_data_path(url):\n",
        "    return os.path.join('./data/', url.split('/')[-1])\n",
        "\n",
        "\n",
        "def download(args):\n",
        "    downloads = [\n",
        "        # Can add other downloads here (e.g., other word vectors)\n",
        "        ('GloVe word vectors', args.glove_url),\n",
        "    ]\n",
        "\n",
        "    for name, url in downloads:\n",
        "        output_path = url_to_data_path(url)\n",
        "        if not os.path.exists(output_path):\n",
        "            print(f'Downloading {name}...')\n",
        "            download_url(url, output_path)\n",
        "\n",
        "        if os.path.exists(output_path) and output_path.endswith('.zip'):\n",
        "            extracted_path = output_path.replace('.zip', '')\n",
        "            if not os.path.exists(extracted_path):\n",
        "                print(f'Unzipping {name}...')\n",
        "                with ZipFile(output_path, 'r') as zip_fh:\n",
        "                    zip_fh.extractall(extracted_path)\n",
        "\n",
        "    print('Downloading spacy language model...')\n",
        "    run(['python', '-m', 'spacy', 'download', 'en'])\n",
        "\n",
        "def word_tokenize(sent):\n",
        "    doc = nlp(sent)\n",
        "    return [token.text for token in doc]\n",
        "\n",
        "\n",
        "def convert_idx(text, tokens):\n",
        "    current = 0\n",
        "    spans = []\n",
        "    for token in tokens:\n",
        "        current = text.find(token, current)\n",
        "        if current < 0:\n",
        "            print(f\"Token {token} cannot be found\")\n",
        "            raise Exception()\n",
        "        spans.append((current, current + len(token)))\n",
        "        current += len(token)\n",
        "    return spans\n",
        "\n",
        "\n",
        "def process_file(filename, data_type, word_counter, char_counter):\n",
        "    print(f\"Pre-processing {data_type} examples...\")\n",
        "    examples = []\n",
        "    eval_examples = {}\n",
        "    total = 0\n",
        "    with open(filename, \"r\") as fh:\n",
        "        source = json.load(fh)\n",
        "        for article in tqdm(source[\"data\"]):\n",
        "            for para in article[\"paragraphs\"]:\n",
        "                context = para[\"context\"].replace(\n",
        "                    \"''\", '\" ').replace(\"``\", '\" ')\n",
        "                context_tokens = word_tokenize(context)\n",
        "                context_chars = [list(token) for token in context_tokens]\n",
        "                spans = convert_idx(context, context_tokens)\n",
        "                for token in context_tokens:\n",
        "                    word_counter[token] += len(para[\"qas\"])\n",
        "                    for char in token:\n",
        "                        char_counter[char] += len(para[\"qas\"])\n",
        "                for qa in para[\"qas\"]:\n",
        "                    total += 1\n",
        "                    ques = qa[\"question\"].replace(\n",
        "                        \"''\", '\" ').replace(\"``\", '\" ')\n",
        "                    ques_tokens = word_tokenize(ques)\n",
        "                    ques_chars = [list(token) for token in ques_tokens]\n",
        "                    for token in ques_tokens:\n",
        "                        word_counter[token] += 1\n",
        "                        for char in token:\n",
        "                            char_counter[char] += 1\n",
        "                    y1s, y2s = [], []\n",
        "                    answer_texts = []\n",
        "                    for answer in qa[\"answers\"]:\n",
        "                        answer_text = answer[\"text\"]\n",
        "                        answer_start = answer['answer_start']\n",
        "                        answer_end = answer_start + len(answer_text)\n",
        "                        answer_texts.append(answer_text)\n",
        "                        answer_span = []\n",
        "                        for idx, span in enumerate(spans):\n",
        "                            if not (answer_end <= span[0] or answer_start >= span[1]):\n",
        "                                answer_span.append(idx)\n",
        "                        y1, y2 = answer_span[0], answer_span[-1]\n",
        "                        y1s.append(y1)\n",
        "                        y2s.append(y2)\n",
        "                    example = {\"context_tokens\": context_tokens,\n",
        "                               \"context_chars\": context_chars,\n",
        "                               \"ques_tokens\": ques_tokens,\n",
        "                               \"ques_chars\": ques_chars,\n",
        "                               \"y1s\": y1s,\n",
        "                               \"y2s\": y2s,\n",
        "                               \"id\": total}\n",
        "                    examples.append(example)\n",
        "                    eval_examples[str(total)] = {\"context\": context,\n",
        "                                                 \"question\": ques,\n",
        "                                                 \"spans\": spans,\n",
        "                                                 \"answers\": answer_texts,\n",
        "                                                 \"uuid\": qa[\"id\"]}\n",
        "        print(f\"{len(examples)} questions in total\")\n",
        "    return examples, eval_examples\n",
        "\n",
        "\n",
        "def get_embedding(counter, data_type, limit=-1, emb_file=None, vec_size=None, num_vectors=None):\n",
        "    print(f\"Pre-processing {data_type} vectors...\")\n",
        "    embedding_dict = {}\n",
        "    filtered_elements = [k for k, v in counter.items() if v > limit]\n",
        "    if emb_file is not None:\n",
        "        assert vec_size is not None\n",
        "        with open(emb_file, \"r\", encoding=\"utf-8\") as fh:\n",
        "            for line in tqdm(fh, total=num_vectors):\n",
        "                array = line.split()\n",
        "                word = \"\".join(array[0:-vec_size])\n",
        "                vector = list(map(float, array[-vec_size:]))\n",
        "                if word in counter and counter[word] > limit:\n",
        "                    embedding_dict[word] = vector\n",
        "        print(f\"{len(embedding_dict)} / {len(filtered_elements)} tokens have corresponding {data_type} embedding vector\")\n",
        "    else:\n",
        "        assert vec_size is not None\n",
        "        for token in filtered_elements:\n",
        "            embedding_dict[token] = [np.random.normal(\n",
        "                scale=0.1) for _ in range(vec_size)]\n",
        "        print(f\"{len(filtered_elements)} tokens have corresponding {data_type} embedding vector\")\n",
        "\n",
        "    NULL = \"--NULL--\"\n",
        "    OOV = \"--OOV--\"\n",
        "    token2idx_dict = {token: idx for idx, token in enumerate(embedding_dict.keys(), 2)}\n",
        "    token2idx_dict[NULL] = 0\n",
        "    token2idx_dict[OOV] = 1\n",
        "    embedding_dict[NULL] = [0. for _ in range(vec_size)]\n",
        "    embedding_dict[OOV] = [0. for _ in range(vec_size)]\n",
        "    idx2emb_dict = {idx: embedding_dict[token]\n",
        "                    for token, idx in token2idx_dict.items()}\n",
        "    emb_mat = [idx2emb_dict[idx] for idx in range(len(idx2emb_dict))]\n",
        "    return emb_mat, token2idx_dict\n",
        "\n",
        "\n",
        "def convert_to_features(args, data, word2idx_dict, char2idx_dict, is_test):\n",
        "    example = {}\n",
        "    context, question = data\n",
        "    context = context.replace(\"''\", '\" ').replace(\"``\", '\" ')\n",
        "    question = question.replace(\"''\", '\" ').replace(\"``\", '\" ')\n",
        "    example['context_tokens'] = word_tokenize(context)\n",
        "    example['ques_tokens'] = word_tokenize(question)\n",
        "    example['context_chars'] = [list(token) for token in example['context_tokens']]\n",
        "    example['ques_chars'] = [list(token) for token in example['ques_tokens']]\n",
        "\n",
        "    para_limit = args.test_para_limit if is_test else args.para_limit\n",
        "    ques_limit = args.test_ques_limit if is_test else args.ques_limit\n",
        "    char_limit = args.char_limit\n",
        "\n",
        "    def filter_func(example):\n",
        "        return len(example[\"context_tokens\"]) > para_limit or \\\n",
        "               len(example[\"ques_tokens\"]) > ques_limit\n",
        "\n",
        "    if filter_func(example):\n",
        "        raise ValueError(\"Context/Questions lengths are over the limit\")\n",
        "\n",
        "    context_idxs = np.zeros([para_limit], dtype=np.int32)\n",
        "    context_char_idxs = np.zeros([para_limit, char_limit], dtype=np.int32)\n",
        "    ques_idxs = np.zeros([ques_limit], dtype=np.int32)\n",
        "    ques_char_idxs = np.zeros([ques_limit, char_limit], dtype=np.int32)\n",
        "\n",
        "    def _get_word(word):\n",
        "        for each in (word, word.lower(), word.capitalize(), word.upper()):\n",
        "            if each in word2idx_dict:\n",
        "                return word2idx_dict[each]\n",
        "        return 1\n",
        "\n",
        "    def _get_char(char):\n",
        "        if char in char2idx_dict:\n",
        "            return char2idx_dict[char]\n",
        "        return 1\n",
        "\n",
        "    for i, token in enumerate(example[\"context_tokens\"]):\n",
        "        context_idxs[i] = _get_word(token)\n",
        "\n",
        "    for i, token in enumerate(example[\"ques_tokens\"]):\n",
        "        ques_idxs[i] = _get_word(token)\n",
        "\n",
        "    for i, token in enumerate(example[\"context_chars\"]):\n",
        "        for j, char in enumerate(token):\n",
        "            if j == char_limit:\n",
        "                break\n",
        "            context_char_idxs[i, j] = _get_char(char)\n",
        "\n",
        "    for i, token in enumerate(example[\"ques_chars\"]):\n",
        "        for j, char in enumerate(token):\n",
        "            if j == char_limit:\n",
        "                break\n",
        "            ques_char_idxs[i, j] = _get_char(char)\n",
        "\n",
        "    return context_idxs, context_char_idxs, ques_idxs, ques_char_idxs\n",
        "\n",
        "\n",
        "def is_answerable(example):\n",
        "    return len(example['y2s']) > 0 and len(example['y1s']) > 0\n",
        "\n",
        "\n",
        "def build_features(args, examples, data_type, out_file, word2idx_dict, char2idx_dict, is_test=False):\n",
        "    para_limit = args.test_para_limit if is_test else args.para_limit\n",
        "    ques_limit = args.test_ques_limit if is_test else args.ques_limit\n",
        "    ans_limit = args.ans_limit\n",
        "    char_limit = args.char_limit\n",
        "\n",
        "    def drop_example(ex, is_test_=False):\n",
        "        if is_test_:\n",
        "            drop = False\n",
        "        else:\n",
        "            drop = len(ex[\"context_tokens\"]) > para_limit or \\\n",
        "                   len(ex[\"ques_tokens\"]) > ques_limit or \\\n",
        "                   (is_answerable(ex) and\n",
        "                    ex[\"y2s\"][0] - ex[\"y1s\"][0] > ans_limit)\n",
        "\n",
        "        return drop\n",
        "\n",
        "    print(f\"Converting {data_type} examples to indices...\")\n",
        "    total = 0\n",
        "    total_ = 0\n",
        "    meta = {}\n",
        "    context_idxs = []\n",
        "    context_char_idxs = []\n",
        "    ques_idxs = []\n",
        "    ques_char_idxs = []\n",
        "    y1s = []\n",
        "    y2s = []\n",
        "    ids = []\n",
        "    for n, example in tqdm(enumerate(examples)):\n",
        "        total_ += 1\n",
        "\n",
        "        if drop_example(example, is_test):\n",
        "            continue\n",
        "\n",
        "        total += 1\n",
        "\n",
        "        def _get_word(word):\n",
        "            for each in (word, word.lower(), word.capitalize(), word.upper()):\n",
        "                if each in word2idx_dict:\n",
        "                    return word2idx_dict[each]\n",
        "            return 1\n",
        "\n",
        "        def _get_char(char):\n",
        "            if char in char2idx_dict:\n",
        "                return char2idx_dict[char]\n",
        "            return 1\n",
        "\n",
        "        context_idx = np.zeros([para_limit], dtype=np.int32)\n",
        "        context_char_idx = np.zeros([para_limit, char_limit], dtype=np.int32)\n",
        "        ques_idx = np.zeros([ques_limit], dtype=np.int32)\n",
        "        ques_char_idx = np.zeros([ques_limit, char_limit], dtype=np.int32)\n",
        "\n",
        "        for i, token in enumerate(example[\"context_tokens\"]):\n",
        "            context_idx[i] = _get_word(token)\n",
        "        context_idxs.append(context_idx)\n",
        "\n",
        "        for i, token in enumerate(example[\"ques_tokens\"]):\n",
        "            ques_idx[i] = _get_word(token)\n",
        "        ques_idxs.append(ques_idx)\n",
        "\n",
        "        for i, token in enumerate(example[\"context_chars\"]):\n",
        "            for j, char in enumerate(token):\n",
        "                if j == char_limit:\n",
        "                    break\n",
        "                context_char_idx[i, j] = _get_char(char)\n",
        "        context_char_idxs.append(context_char_idx)\n",
        "\n",
        "        for i, token in enumerate(example[\"ques_chars\"]):\n",
        "            for j, char in enumerate(token):\n",
        "                if j == char_limit:\n",
        "                    break\n",
        "                ques_char_idx[i, j] = _get_char(char)\n",
        "        ques_char_idxs.append(ques_char_idx)\n",
        "\n",
        "        if is_answerable(example):\n",
        "            start, end = example[\"y1s\"][-1], example[\"y2s\"][-1]\n",
        "        else:\n",
        "            start, end = -1, -1\n",
        "\n",
        "        y1s.append(start)\n",
        "        y2s.append(end)\n",
        "        ids.append(example[\"id\"])\n",
        "\n",
        "    np.savez(out_file,\n",
        "             context_idxs=np.array(context_idxs),\n",
        "             context_char_idxs=np.array(context_char_idxs),\n",
        "             ques_idxs=np.array(ques_idxs),\n",
        "             ques_char_idxs=np.array(ques_char_idxs),\n",
        "             y1s=np.array(y1s),\n",
        "             y2s=np.array(y2s),\n",
        "             ids=np.array(ids))\n",
        "    print(f\"Built {total} / {total_} instances of features in total\")\n",
        "    meta[\"total\"] = total\n",
        "    return meta\n",
        "\n",
        "\n",
        "def save(filename, obj, message=None):\n",
        "    if message is not None:\n",
        "        print(f\"Saving {message}...\")\n",
        "        with open(filename, \"w\") as fh:\n",
        "            json.dump(obj, fh)\n",
        "\n",
        "\n",
        "def pre_process(args):\n",
        "    # Process training set and use it to decide on the word/character vocabularies\n",
        "    word_counter, char_counter = Counter(), Counter()\n",
        "    train_examples, train_eval = process_file(args.train_file, \"train\", word_counter, char_counter)\n",
        "    word_emb_mat, word2idx_dict = get_embedding(\n",
        "        word_counter, 'word', emb_file=args.glove_file, vec_size=args.glove_dim, num_vectors=args.glove_num_vecs)\n",
        "    char_emb_mat, char2idx_dict = get_embedding(\n",
        "        char_counter, 'char', emb_file=None, vec_size=args.char_dim)\n",
        "\n",
        "    # Process dev and test sets\n",
        "    dev_examples, dev_eval = process_file(args.dev_file, \"dev\", word_counter, char_counter)\n",
        "    build_features(args, train_examples, \"train\", args.train_record_file, word2idx_dict, char2idx_dict)\n",
        "    dev_meta = build_features(args, dev_examples, \"dev\", args.dev_record_file, word2idx_dict, char2idx_dict)\n",
        "    if args.include_test_examples:\n",
        "        test_examples, test_eval = process_file(args.test_file, \"test\", word_counter, char_counter)\n",
        "        save(args.test_eval_file, test_eval, message=\"test eval\")\n",
        "        test_meta = build_features(args, test_examples, \"test\",\n",
        "                                   args.test_record_file, word2idx_dict, char2idx_dict, is_test=True)\n",
        "        save(args.test_meta_file, test_meta, message=\"test meta\")\n",
        "\n",
        "    save(args.word_emb_file, word_emb_mat, message=\"word embedding\")\n",
        "    save(args.char_emb_file, char_emb_mat, message=\"char embedding\")\n",
        "    save(args.train_eval_file, train_eval, message=\"train eval\")\n",
        "    save(args.dev_eval_file, dev_eval, message=\"dev eval\")\n",
        "    save(args.word2idx_file, word2idx_dict, message=\"word dictionary\")\n",
        "    save(args.char2idx_file, char2idx_dict, message=\"char dictionary\")\n",
        "    save(args.dev_meta_file, dev_meta, message=\"dev meta\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Get command-line args\n",
        "    args_ = get_setup_args()\n",
        "\n",
        "    # Download resources\n",
        "    download(args_)\n",
        "\n",
        "    # Import spacy language model\n",
        "    nlp = spacy.blank(\"en\")\n",
        "\n",
        "    # Preprocess dataset\n",
        "    args_.train_file = url_to_data_path(args_.train_url)\n",
        "    args_.dev_file = url_to_data_path(args_.dev_url)\n",
        "    if args_.include_test_examples:\n",
        "        args_.test_file = url_to_data_path(args_.test_url)\n",
        "    glove_dir = url_to_data_path(args_.glove_url.replace('.zip', ''))\n",
        "    glove_ext = f'.txt' if glove_dir.endswith('d') else f'.{args_.glove_dim}d.txt'\n",
        "    args_.glove_file = os.path.join(glove_dir, os.path.basename(glove_dir) + glove_ext)\n",
        "    pre_process(args_)"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}