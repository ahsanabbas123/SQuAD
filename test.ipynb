{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test a model and generate submission CSV.\n",
        "\n",
        "Usage:\n",
        "    > python test.py --split SPLIT --load_path PATH --name NAME\n",
        "    where\n",
        "    > SPLIT is either \"dev\" or \"test\"\n",
        "    > PATH is a path to a checkpoint (e.g., save/train/model-01/best.pth.tar)\n",
        "    > NAME is a name to identify the test run\n",
        "\n",
        "Author:\n",
        "    Chris Chute (chute@stanford.edu)\n",
        "\"\"\"\n",
        "\n",
        "import csv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import util\n",
        "\n",
        "from args import get_test_args\n",
        "from collections import OrderedDict\n",
        "from json import dumps\n",
        "from models import BiDAF\n",
        "from os.path import join\n",
        "from tensorboardX import SummaryWriter\n",
        "from tqdm import tqdm\n",
        "from ujson import load as json_load\n",
        "from util import collate_fn, SQuAD\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    # Set up logging\n",
        "    args.save_dir = util.get_save_dir(args.save_dir, args.name, training=False)\n",
        "    log = util.get_logger(args.save_dir, args.name)\n",
        "    log.info(f'Args: {dumps(vars(args), indent=4, sort_keys=True)}')\n",
        "    device, gpu_ids = util.get_available_devices()\n",
        "    args.batch_size *= max(1, len(gpu_ids))\n",
        "\n",
        "    # Get embeddings\n",
        "    log.info('Loading embeddings...')\n",
        "    word_vectors = util.torch_from_json(args.word_emb_file)\n",
        "\n",
        "    # Get model\n",
        "    log.info('Building model...')\n",
        "    model = BiDAF(word_vectors=word_vectors,\n",
        "                  hidden_size=args.hidden_size)\n",
        "    model = nn.DataParallel(model, gpu_ids)\n",
        "    log.info(f'Loading checkpoint from {args.load_path}...')\n",
        "    model = util.load_model(model, args.load_path, gpu_ids, return_step=False)\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Get data loader\n",
        "    log.info('Building dataset...')\n",
        "    record_file = vars(args)[f'{args.split}_record_file']\n",
        "    dataset = SQuAD(record_file, args.use_squad_v2)\n",
        "    data_loader = data.DataLoader(dataset,\n",
        "                                  batch_size=args.batch_size,\n",
        "                                  shuffle=False,\n",
        "                                  num_workers=args.num_workers,\n",
        "                                  collate_fn=collate_fn)\n",
        "\n",
        "    # Evaluate\n",
        "    log.info(f'Evaluating on {args.split} split...')\n",
        "    nll_meter = util.AverageMeter()\n",
        "    pred_dict = {}  # Predictions for TensorBoard\n",
        "    sub_dict = {}   # Predictions for submission\n",
        "    eval_file = vars(args)[f'{args.split}_eval_file']\n",
        "    with open(eval_file, 'r') as fh:\n",
        "        gold_dict = json_load(fh)\n",
        "    with torch.no_grad(), \\\n",
        "            tqdm(total=len(dataset)) as progress_bar:\n",
        "        for cw_idxs, cc_idxs, qw_idxs, qc_idxs, y1, y2, ids in data_loader:\n",
        "            # Setup for forward\n",
        "            cw_idxs = cw_idxs.to(device)\n",
        "            qw_idxs = qw_idxs.to(device)\n",
        "            batch_size = cw_idxs.size(0)\n",
        "\n",
        "            # Forward\n",
        "            log_p1, log_p2 = model(cw_idxs, qw_idxs)\n",
        "            y1, y2 = y1.to(device), y2.to(device)\n",
        "            loss = F.nll_loss(log_p1, y1) + F.nll_loss(log_p2, y2)\n",
        "            nll_meter.update(loss.item(), batch_size)\n",
        "\n",
        "            # Get F1 and EM scores\n",
        "            p1, p2 = log_p1.exp(), log_p2.exp()\n",
        "            starts, ends = util.discretize(p1, p2, args.max_ans_len, args.use_squad_v2)\n",
        "\n",
        "            # Log info\n",
        "            progress_bar.update(batch_size)\n",
        "            if args.split != 'test':\n",
        "                # No labels for the test set, so NLL would be invalid\n",
        "                progress_bar.set_postfix(NLL=nll_meter.avg)\n",
        "\n",
        "            idx2pred, uuid2pred = util.convert_tokens(gold_dict,\n",
        "                                                      ids.tolist(),\n",
        "                                                      starts.tolist(),\n",
        "                                                      ends.tolist(),\n",
        "                                                      args.use_squad_v2)\n",
        "            pred_dict.update(idx2pred)\n",
        "            sub_dict.update(uuid2pred)\n",
        "\n",
        "    # Log results (except for test set, since it does not come with labels)\n",
        "    if args.split != 'test':\n",
        "        results = util.eval_dicts(gold_dict, pred_dict, args.use_squad_v2)\n",
        "        results_list = [('NLL', nll_meter.avg),\n",
        "                        ('F1', results['F1']),\n",
        "                        ('EM', results['EM'])]\n",
        "        if args.use_squad_v2:\n",
        "            results_list.append(('AvNA', results['AvNA']))\n",
        "        results = OrderedDict(results_list)\n",
        "\n",
        "        # Log to console\n",
        "        results_str = ', '.join(f'{k}: {v:05.2f}' for k, v in results.items())\n",
        "        log.info(f'{args.split.title()} {results_str}')\n",
        "\n",
        "        # Log to TensorBoard\n",
        "        tbx = SummaryWriter(args.save_dir)\n",
        "        util.visualize(tbx,\n",
        "                       pred_dict=pred_dict,\n",
        "                       eval_path=eval_file,\n",
        "                       step=0,\n",
        "                       split=args.split,\n",
        "                       num_visuals=args.num_visuals)\n",
        "\n",
        "    # Write submission file\n",
        "    sub_path = join(args.save_dir, args.split + '_' + args.sub_file)\n",
        "    log.info(f'Writing submission file to {sub_path}...')\n",
        "    with open(sub_path, 'w', newline='', encoding='utf-8') as csv_fh:\n",
        "        csv_writer = csv.writer(csv_fh, delimiter=',')\n",
        "        csv_writer.writerow(['Id', 'Predicted'])\n",
        "        for uuid in sorted(sub_dict):\n",
        "            csv_writer.writerow([uuid, sub_dict[uuid]])\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main(get_test_args())"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}