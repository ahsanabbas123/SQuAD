{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train a model on SQuAD.\n",
        "\n",
        "Author:\n",
        "    Chris Chute (chute@stanford.edu)\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as sched\n",
        "import torch.utils.data as data\n",
        "import util\n",
        "\n",
        "from args import get_train_args\n",
        "from collections import OrderedDict\n",
        "from json import dumps\n",
        "from models import BiDAF\n",
        "from tensorboardX import SummaryWriter\n",
        "from tqdm import tqdm\n",
        "from ujson import load as json_load\n",
        "from util import collate_fn, SQuAD\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    # Set up logging and devices\n",
        "    args.save_dir = util.get_save_dir(args.save_dir, args.name, training=True)\n",
        "    log = util.get_logger(args.save_dir, args.name)\n",
        "    tbx = SummaryWriter(args.save_dir)\n",
        "    device, args.gpu_ids = util.get_available_devices()\n",
        "    log.info(f'Args: {dumps(vars(args), indent=4, sort_keys=True)}')\n",
        "    args.batch_size *= max(1, len(args.gpu_ids))\n",
        "\n",
        "    # Set random seed\n",
        "    log.info(f'Using random seed {args.seed}...')\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "    # Get embeddings\n",
        "    log.info('Loading embeddings...')\n",
        "    word_vectors = util.torch_from_json(args.word_emb_file)\n",
        "\n",
        "    # Get model\n",
        "    log.info('Building model...')\n",
        "    model = BiDAF(word_vectors=word_vectors,\n",
        "                  hidden_size=args.hidden_size,\n",
        "                  drop_prob=args.drop_prob)\n",
        "    model = nn.DataParallel(model, args.gpu_ids)\n",
        "    if args.load_path:\n",
        "        log.info(f'Loading checkpoint from {args.load_path}...')\n",
        "        model, step = util.load_model(model, args.load_path, args.gpu_ids)\n",
        "    else:\n",
        "        step = 0\n",
        "    model = model.to(device)\n",
        "    model.train()\n",
        "    ema = util.EMA(model, args.ema_decay)\n",
        "\n",
        "    # Get saver\n",
        "    saver = util.CheckpointSaver(args.save_dir,\n",
        "                                 max_checkpoints=args.max_checkpoints,\n",
        "                                 metric_name=args.metric_name,\n",
        "                                 maximize_metric=args.maximize_metric,\n",
        "                                 log=log)\n",
        "\n",
        "    # Get optimizer and scheduler\n",
        "    optimizer = optim.Adadelta(model.parameters(), args.lr,\n",
        "                               weight_decay=args.l2_wd)\n",
        "    scheduler = sched.LambdaLR(optimizer, lambda s: 1.)  # Constant LR\n",
        "\n",
        "    # Get data loader\n",
        "    log.info('Building dataset...')\n",
        "    train_dataset = SQuAD(args.train_record_file, args.use_squad_v2)\n",
        "    train_loader = data.DataLoader(train_dataset,\n",
        "                                   batch_size=args.batch_size,\n",
        "                                   shuffle=True,\n",
        "                                   num_workers=args.num_workers,\n",
        "                                   collate_fn=collate_fn)\n",
        "    dev_dataset = SQuAD(args.dev_record_file, args.use_squad_v2)\n",
        "    dev_loader = data.DataLoader(dev_dataset,\n",
        "                                 batch_size=args.batch_size,\n",
        "                                 shuffle=False,\n",
        "                                 num_workers=args.num_workers,\n",
        "                                 collate_fn=collate_fn)\n",
        "\n",
        "    # Train\n",
        "    log.info('Training...')\n",
        "    steps_till_eval = args.eval_steps\n",
        "    epoch = step // len(train_dataset)\n",
        "    while epoch != args.num_epochs:\n",
        "        epoch += 1\n",
        "        log.info(f'Starting epoch {epoch}...')\n",
        "        with torch.enable_grad(), \\\n",
        "                tqdm(total=len(train_loader.dataset)) as progress_bar:\n",
        "            for cw_idxs, cc_idxs, qw_idxs, qc_idxs, y1, y2, ids in train_loader:\n",
        "                # Setup for forward\n",
        "                cw_idxs = cw_idxs.to(device)\n",
        "                qw_idxs = qw_idxs.to(device)\n",
        "                batch_size = cw_idxs.size(0)\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward\n",
        "                log_p1, log_p2 = model(cw_idxs, qw_idxs)\n",
        "                y1, y2 = y1.to(device), y2.to(device)\n",
        "                loss = F.nll_loss(log_p1, y1) + F.nll_loss(log_p2, y2)\n",
        "                loss_val = loss.item()\n",
        "\n",
        "                # Backward\n",
        "                loss.backward()\n",
        "                nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "                optimizer.step()\n",
        "                scheduler.step(step // batch_size)\n",
        "                ema(model, step // batch_size)\n",
        "\n",
        "                # Log info\n",
        "                step += batch_size\n",
        "                progress_bar.update(batch_size)\n",
        "                progress_bar.set_postfix(epoch=epoch,\n",
        "                                         NLL=loss_val)\n",
        "                tbx.add_scalar('train/NLL', loss_val, step)\n",
        "                tbx.add_scalar('train/LR',\n",
        "                               optimizer.param_groups[0]['lr'],\n",
        "                               step)\n",
        "\n",
        "                steps_till_eval -= batch_size\n",
        "                if steps_till_eval <= 0:\n",
        "                    steps_till_eval = args.eval_steps\n",
        "\n",
        "                    # Evaluate and save checkpoint\n",
        "                    log.info(f'Evaluating at step {step}...')\n",
        "                    ema.assign(model)\n",
        "                    results, pred_dict = evaluate(model, dev_loader, device,\n",
        "                                                  args.dev_eval_file,\n",
        "                                                  args.max_ans_len,\n",
        "                                                  args.use_squad_v2)\n",
        "                    saver.save(step, model, results[args.metric_name], device)\n",
        "                    ema.resume(model)\n",
        "\n",
        "                    # Log to console\n",
        "                    results_str = ', '.join(f'{k}: {v:05.2f}' for k, v in results.items())\n",
        "                    log.info(f'Dev {results_str}')\n",
        "\n",
        "                    # Log to TensorBoard\n",
        "                    log.info('Visualizing in TensorBoard...')\n",
        "                    for k, v in results.items():\n",
        "                        tbx.add_scalar(f'dev/{k}', v, step)\n",
        "                    util.visualize(tbx,\n",
        "                                   pred_dict=pred_dict,\n",
        "                                   eval_path=args.dev_eval_file,\n",
        "                                   step=step,\n",
        "                                   split='dev',\n",
        "                                   num_visuals=args.num_visuals)\n",
        "\n",
        "\n",
        "def evaluate(model, data_loader, device, eval_file, max_len, use_squad_v2):\n",
        "    nll_meter = util.AverageMeter()\n",
        "\n",
        "    model.eval()\n",
        "    pred_dict = {}\n",
        "    with open(eval_file, 'r') as fh:\n",
        "        gold_dict = json_load(fh)\n",
        "    with torch.no_grad(), \\\n",
        "            tqdm(total=len(data_loader.dataset)) as progress_bar:\n",
        "        for cw_idxs, cc_idxs, qw_idxs, qc_idxs, y1, y2, ids in data_loader:\n",
        "            # Setup for forward\n",
        "            cw_idxs = cw_idxs.to(device)\n",
        "            qw_idxs = qw_idxs.to(device)\n",
        "            batch_size = cw_idxs.size(0)\n",
        "\n",
        "            # Forward\n",
        "            log_p1, log_p2 = model(cw_idxs, qw_idxs)\n",
        "            y1, y2 = y1.to(device), y2.to(device)\n",
        "            loss = F.nll_loss(log_p1, y1) + F.nll_loss(log_p2, y2)\n",
        "            nll_meter.update(loss.item(), batch_size)\n",
        "\n",
        "            # Get F1 and EM scores\n",
        "            p1, p2 = log_p1.exp(), log_p2.exp()\n",
        "            starts, ends = util.discretize(p1, p2, max_len, use_squad_v2)\n",
        "\n",
        "            # Log info\n",
        "            progress_bar.update(batch_size)\n",
        "            progress_bar.set_postfix(NLL=nll_meter.avg)\n",
        "\n",
        "            preds, _ = util.convert_tokens(gold_dict,\n",
        "                                           ids.tolist(),\n",
        "                                           starts.tolist(),\n",
        "                                           ends.tolist(),\n",
        "                                           use_squad_v2)\n",
        "            pred_dict.update(preds)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    results = util.eval_dicts(gold_dict, pred_dict, use_squad_v2)\n",
        "    results_list = [('NLL', nll_meter.avg),\n",
        "                    ('F1', results['F1']),\n",
        "                    ('EM', results['EM'])]\n",
        "    if use_squad_v2:\n",
        "        results_list.append(('AvNA', results['AvNA']))\n",
        "    results = OrderedDict(results_list)\n",
        "\n",
        "    return results, pred_dict\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main(get_train_args())"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}